{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Income Correlated variables based on Panel Study of Income Dynamics(PSID) data from 1992 to 2012\n",
    "## Content\n",
    "- [Introduction](#Introduction)\n",
    "- [Data Cleanning Based on Distribution Analysis](#Data-Cleanning-Based-on-Distribution-Analysis)\n",
    "- [Correlation Analysis Based on Scatter Plot and Linear Regression](#Correlation-Analysis-Based-on-Scatter-Plot-and-Linear-Regression)\n",
    "- [Engels Coefficient Analysis](#Engels-Coefficient-Analysis)\n",
    "- [Gini Coefficient Analysis](#Gini-Coefficient-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the final project, we investigate the correlation between household-level income and our selected income shocks (interesed varaibles) separately by our cleaning PSID(Panel Study of Income Dynamics) data. In particular, we've done a lot of work in cleaning PSID raw data with R in the beginning, and explore the correlation base on the scatter plot and linear regression of PSID cleaning data. Moreover, we verify the Engels coefficient analysis and Gini coefficient analysis with our cleaning data.\n",
    "\n",
    "As future steps after midterm report, we propose to try several models（eg. linear regression）to fit the relationship between household-level income and income shocks based on our observation. Then, we will split PSID data into three parts: testing data, cross-validation data, and training data based on years.We will predict the household-level income scale and the growth of household-level income with PSID testing data(2010 & 2012) and also evaluate how well our model predict the new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanning Based on Distribution Analysis\n",
    "It is important to clean data before we start to do actual analysis on it. To be specific, we first select variables that we are interested in and then do data cleaning for the selected variables one by one.\n",
    "\n",
    "### Selecting Interesting Variables\n",
    "In this section, we import necessary libraries, read raw data from csv, select data that happens after 1992. We do this selection based on year because we want to focus on data that happens not too far away. Then we select the following interesting variables that we think is closely related to one’s income.\n",
    "\n",
    "1. house: the price of one's house\n",
    "2. year: the year to conduct survey\n",
    "3. rent: annual rent of a family\n",
    "4. food: the money spent eating at home\n",
    "5. fout: the money spent eating outside\n",
    "6. hhelp: the amount of help received from the relatives/friends of the husband of this family\n",
    "7. whelp: the amount of help received from the relatives/friends of the husband of this family\n",
    "8. marit: marital status\n",
    "9. educ: education year\n",
    "10. kids: number of kids a family has\n",
    "11. race: race \n",
    "12. y: total annual income of a family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn # or alternatively plt.style.use('ggplot') for a similar look\n",
    "\n",
    "matplotlib.rc(\"figure\", figsize=(8,6))\n",
    "matplotlib.rc(\"axes\", labelsize=16, titlesize=16)\n",
    "matplotlib.rc(\"xtick\", labelsize=14)\n",
    "matplotlib.rc(\"ytick\", labelsize=14)\n",
    "matplotlib.rc(\"legend\", fontsize=14)\n",
    "matplotlib.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File data_1968_2013_Updated.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c22eb3c333e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_1968_2013_Updated.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# focus on recent data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m1992\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File data_1968_2013_Updated.csv does not exist"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data_1968_2013_Updated.csv')\n",
    "\n",
    "# focus on recent data\n",
    "data = df[df['year']>=1992].sort_values(by=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get basic information of data\n",
    "print data.head()\n",
    "print data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select interesting variables\n",
    "data = data[['id','person','year','house','rent','food','fout','hhelp','whelp','marit','educ','kids','race', 'y']]\n",
    "data['income']= data['y']\n",
    "del data['y']\n",
    "print data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove . and space in data\n",
    "data = data.replace('', np.nan)\n",
    "data = data.replace('.', np.nan)\n",
    "data = data.dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning for Interesting Variables\n",
    "Having selected interesting variables, we clean the values for them one by one. Generally speaking three aspects are involved here. \n",
    "1. Type conversion, where it is necessary to convert string to integer or float. \n",
    "2. Outlier(abnormal value) removal. We adopt the hist graph to detect outliers and abnormal values, and remove a row if this row contains at least one abnormal value. \n",
    "3. Convert other useful values to the log scale of some original value.\n",
    "\n",
    "In each of the following code cells we marked clearly what is the variable we are cleaning and why we are removing some values. Please refer to the code, comments, and hist graphs for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean education\n",
    "plt.figure(0)\n",
    "data['educ'].hist(bins = 30) #education distribution before cleaning\n",
    "\n",
    "data = data[data['educ'] != 99]\n",
    "plt.figure(1)\n",
    "data['educ'].hist(bins = 30) #education distribution after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean rent: find outliers\n",
    "plt.figure(0)\n",
    "data['rent'] = data['rent'].astype(float) \n",
    "data['rent'].hist(bins = 30) #from this plot we start to use log scale\n",
    "\n",
    "plt.figure(1)\n",
    "rent = data['rent']\n",
    "log_rent = rent[rent > 0].map(lambda x : math.log(x))\n",
    "log_rent.hist(bins = 30) # rent distribution before cleaning\n",
    "#print log_rent[log_rent>12]\n",
    "\n",
    "data = data[data['rent'] < 1000000] # clean data\n",
    "\n",
    "plt.figure(2)\n",
    "rent = data['rent']\n",
    "log_rent = rent[rent > 0].map(lambda x : math.log(x))\n",
    "log_rent.hist(bins = 30) # rent distribution after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows an interesting distribution of the rents among all the data. The people who have houses have a rent value 0, which should be ignored at this plot. However, the rest of the histogram shows that a small portion of families have a much higher rent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean house\n",
    "plt.figure(0)\n",
    "house = data['house']\n",
    "log_house = house[house > 0].map(lambda x : math.log(x))\n",
    "#print house[house > 8e6].head\n",
    "log_house.hist(bins = 30) # house value distribution before cleaning\n",
    "\n",
    "data = data[data['house'] < 8e6]\n",
    "\n",
    "plt.figure(1)\n",
    "house = data['house']\n",
    "log_house = house[house > 0].map(lambda x : math.log(x))\n",
    "log_house.hist(bins = 30) # house value distribution after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean food\n",
    "data['food']= data['food'].astype(float)\n",
    "plt.figure(0)\n",
    "food = data['food']\n",
    "print food[food > 1e6].head()\n",
    "# Because all the food consumption are distinct, we assume they are reasonble large data.\n",
    "\n",
    "food.hist(bins = 30) # food distribution before cleaning\n",
    "log_food = food[food > 0].map(lambda x : math.log(x))\n",
    "plt.figure(1)\n",
    "log_food.hist(bins = 30) # food distribution after log scale\n",
    "print food[food>math.exp(14)].head()\n",
    "data = data[data['food']<math.exp(14)]\n",
    "food = data['food']\n",
    "log_food = food[food > 0].map(lambda x : math.log(x))\n",
    "plt.figure(2)\n",
    "log_food.hist(bins = 30) # food distribution after log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean fout(cost of delivered food)\n",
    "data['fout'] = data['fout'].astype(float)\n",
    "plt.figure(0)\n",
    "fout = data['fout']\n",
    "print fout[fout > 1e6].head()\n",
    "# Because all the food consumption are almost the same, we assume they are not reasonble large data.\n",
    "\n",
    "fout.hist(bins = 30) # fout distribution before cleaning\n",
    "data = data[data['fout'] < 1e6]\n",
    "fout = data['fout']\n",
    "log_fout = fout[fout > 0].map(lambda x : math.log(x))\n",
    "plt.figure(1)\n",
    "log_food.hist(bins = 30) # food distribution after log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean hhelp(get help from husband)\n",
    "data['hhelp'] = data['hhelp'].astype(float)\n",
    "plt.figure(0)\n",
    "hhelp = data['hhelp']\n",
    "print hhelp[hhelp > 8e5].head()\n",
    "# Because all the food consumption are almost the same, we assume they are not reasonble large data.\n",
    "\n",
    "hhelp.hist(bins = 30) # hhelp distribution before cleaning\n",
    "data = data[data['hhelp'] < 8e5]\n",
    "plt.figure(1)\n",
    "hhelp = data['hhelp']\n",
    "hhelp.hist(bins = 30) # hhelp distribution after 1st cleaning\n",
    "print hhelp[hhelp > 8e4].head()\n",
    "# Because all the food consumption are almost the same, we assume they are not reasonble large data.\n",
    "data = data[data['hhelp'] < 8e4]\n",
    "plt.figure(2)\n",
    "hhelp = data['hhelp']\n",
    "hhelp.hist(bins = 30) # hhelp distribution after 2nd cleaning\n",
    "log_hhelp = hhelp[hhelp > 0].map(lambda x : math.log(x))\n",
    "plt.figure(3)\n",
    "log_hhelp.hist(bins = 30) # hhelp distribution after log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean whelp(get help from wife)\n",
    "data['whelp'] = data['whelp'].astype(float)\n",
    "plt.figure(0)\n",
    "whelp = data['whelp']\n",
    "whelp.hist(bins = 30) # whelp distribution before cleaning\n",
    "\n",
    "log_whelp = whelp[whelp > 0].map(lambda x : math.log(x))\n",
    "plt.figure(1)\n",
    "log_whelp.hist(bins = 30) # whelp distribution after log scale\n",
    "print whelp[whelp > math.exp(12)].head()\n",
    "data = data[data['whelp'] < math.exp(12)]\n",
    "whelp = data['whelp']\n",
    "log_whelp = whelp[whelp > 0].map(lambda x : math.log(x))\n",
    "plt.figure(2)\n",
    "log_whelp.hist(bins = 30) # whelp distribution after log scale\n",
    "print whelp.sort_values().tail() # Because all the whelp are almost the same, we assume they are not reasonble large data.\n",
    "whelp = whelp[whelp < 99998]\n",
    "print whelp.sort_values().tail() # Because all the whelp are distinct, we assume they are not reasonble large data.\n",
    "data = data[data['whelp'] < 99998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean kids\n",
    "kids = data['kids']\n",
    "plt.figure(0)\n",
    "kids.hist(bins = 30)\n",
    "kids.describe() #looks good, does not need clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean marit\n",
    "# Codes for marital status\n",
    "# 1. Married or permanently cohabiting; wife, \"wife,\" or husband is present in the FU\n",
    "# 2. Single, never legally married and no wife, \"wife,\" or husband is present in the FU\n",
    "# 3. Widowed and no wife, \"wife,\" or husband is present in the FU\n",
    "# 4. Divorced and no wife, \"wife,\" or husband is present in the FU\n",
    "# 5. Separated; legally married but no wife, \"wife,\" or husband is present in the FU (the spouse may be in an institution)\n",
    "\n",
    "(cited from PSID code book)\n",
    "marit = data['marit']\n",
    "marit.hist(bins = 10) #looks good, does not need clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean income\n",
    "income = data['income']\n",
    "plt.figure(0)\n",
    "income.hist(bins = 30)\n",
    "#print income[income>6e6].head()\n",
    "log_income = income[income > 0].map(lambda x : math.log(x))\n",
    "plt.figure(1)\n",
    "log_income.hist(bins = 30) # whelp distribution after log scale\n",
    "print income[income>0].sort_values().head()\n",
    "data = data [data['income']>math.exp(4)]\n",
    "income = data['income']\n",
    "log_income = income[income > 0].map(lambda x : math.log(x))\n",
    "plt.figure(2)\n",
    "log_income.hist(bins = 30) # whelp distribution after log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean race\n",
    "race = data['race']\n",
    "plt.figure(0)\n",
    "race.hist(bins = 30) #looks good, do not need to clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Correlation-Analysis-Based-on-Scatter-Plot-and-Linear-Regression\"></a>\n",
    "## Correlation Analysis Based on Scatter Plot and Correlation\n",
    "\n",
    "From the observation of income and food columns and intuition, we started by looking at plain scatter plots.\n",
    "### Income vs Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(data['income'],data['food'],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log(Income) vs Log(Food)\n",
    "The plain scatter plot is not friendly to visualize. So we decide to get the logs of both income and food consumptions.\n",
    "#### Correlation Comparison: Log(Income) vs Log(Food_at_home) and Log(Income) vs Log(Food_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['log_income'] = data['income'].map(lambda x : math.log(x))\n",
    "data['log_food'] = data['food'].map(lambda x : math.log(x+1))\n",
    "data['log_fout'] = data['fout'].map(lambda x : math.log(x+1))\n",
    "plt.figure(0)\n",
    "plt.plot(data['log_income'],data['log_food'],'.')\n",
    "#corr_pd = pd.DataFrame(data['income','food'])\n",
    "print data[['log_food','log_income']].corr()\n",
    "\n",
    "print data[['log_fout','log_income']].corr()\n",
    "plt.figure(1)\n",
    "plt.plot(data['log_income'],data['log_fout'],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Engels Coefficient Analysis\"></a>\n",
    "## Engels Coefficient Analysis\n",
    "Engel's law is an observation in economics stating that as income rises, the proportion of income spent on food falls, even if actual expenditure on food rises. In other words, the income elasticity of demand of food is between 0 and 1.\n",
    "\n",
    "\n",
    "<img src='engels.png'>\n",
    "\n",
    "One application of this statistic is treating it as a reflection of the living standard of a country. As this proportion or \"Engel coefficient\" increases, the country is by nature poorer, conversely a low Engel coefficient indicates a higher standard of living.\n",
    "\n",
    "(Cited from Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engel coefficient vs log scale income\n",
    "Following plot shows a general observation that people with higher income tends to have lower engel coeffcient (spending less proportion of income on food consumption). It agrees with economics intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_engels = data.copy(deep=True)\n",
    "data_engels =data_engels[data_engels['income']>0]\n",
    "data_engels = data_engels[data_engels['food']+data_engels['fout']<data_engels['income']]\n",
    "def f(x):\n",
    "    return float(x['fout']+x['food'])/x['income']\n",
    "Engels = data_engels.apply(f, axis=1)\n",
    "data_engels['log_income'] = data_engels['income'].map(lambda x : math.log(x))\n",
    "plt.figure(0)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(data_engels['log_income'],Engels,'.')\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist2d(data_engels['log_income'],Engels, bins=100);\n",
    "plt.set_cmap(plt.cm.get_cmap('hot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a class=\"anchor\" id=\"Gini Coefficient Analysis\"></a>\n",
    "## Gini Coefficient Analysis\n",
    "We started by looking at the whisk box charts of the income distributions from 1992 to 2012 (The survey data is collected once per two years). The log_income slightly went up and the middle part (height of the box) remained almost the same. We could not conclude too much on the distribution of income and its changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = seaborn.boxplot(x=\"year\", y=\"log_income\", data=data,whis=[1,99], showfliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of Gini Coefficient\n",
    "To make more sense of how the income distribution changes, we introduce the concept of \"Gini Coefficient\".\n",
    "The Gini Coefficient is one way to measure how evenly the income (or wealth) is distributed throughout a country.\n",
    "\n",
    "The Gini Coefficient is calculated as follows. We find out the income of all the people in a country and then express this information as a cumulative percentage of people against the cumulative share of income earned. This gives us a Lorenz Curve which typically looks something like the following\n",
    "\n",
    "<img src=\"Gini_coefficient.gif\">\n",
    "\n",
    "In plain English, the graph above indicates the proportion of the income going to the poorest people, middle-income people and richest people.\n",
    "\n",
    "<img src=\"gini.gif\">\n",
    "\n",
    "Cited from: http://www.intmath.com/blog/mathematics/the-gini-coefficient-of-wealth-distribution-4187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Gini Coefficient from 1992 to 2012\n",
    "\n",
    "Applying the gini functions to the year grouped data, we found the Gini Coefficient is increasing mostly during the last twenty years, which means the inequality of income is getting worse and worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gini(list_of_values):\n",
    "    sorted_list = sorted(list_of_values)\n",
    "    height, area = 0, 0\n",
    "    for value in sorted_list:\n",
    "        height += value\n",
    "        area += height - value / 2.\n",
    "    fair_area = height * len(list_of_values) / 2.\n",
    "    return (fair_area - area) / fair_area\n",
    "# Cited from: http://planspace.org/2013/06/21/how-to-calculate-gini-coefficient-from-raw-data-in-python/\n",
    "#print gini(data[data['year']==1992.0]['income'])\n",
    "data_gini = data.copy(deep=True)\n",
    "gini = data_gini.groupby(\"year\").income.apply(gini)\n",
    "print data_gini['year'].unique().tolist()\n",
    "print gini\n",
    "plt.plot(data_gini['year'].unique().tolist(),gini,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
